{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.9' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: 'c:/Users/Pradip/AppData/Local/Microsoft/WindowsApps/python3.11.exe -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import json\n",
    "from pathlib import Path\n",
    "from os.path import join as path_join\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from torch.nn import Conv2d\n",
    "from torch import FloatTensor, LongTensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path('C:/Users/MUKTA PATIL/projects/ml/abstraction-and-reasoning-challenge')\n",
    "training_path = data_path / 'training'\n",
    "evaluation_path = data_path / 'evaluation'\n",
    "test_path = data_path / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    # Initialize an empty Series with object dtype\n",
    "    tasks = pd.Series(dtype='object')\n",
    "    for file_name in os.listdir(path):\n",
    "        task_file = path_join(path, file_name)\n",
    "        if os.path.isfile(task_file) and file_name.endswith('.json'):\n",
    "            try:\n",
    "                # Open and load the JSON content\n",
    "                with open(task_file, 'r') as f:\n",
    "                    task = json.load(f)\n",
    "                tasks[file_name[:-5]] = task\n",
    "            except (json.JSONDecodeError, IOError) as e:\n",
    "                # Handle errors gracefully\n",
    "                print(f\"Error loading file {file_name}: {e}\")\n",
    "    \n",
    "    return tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tasks = load_data(training_path)\n",
    "#print(train_tasks.head())\n",
    "def inp2img(inp):\n",
    "    inp = np.array(inp)\n",
    "    img = np.full((10, inp.shape[0], inp.shape[1]), 0, dtype=np.uint8)\n",
    "    for i in range(10):\n",
    "        img[i] = (inp==i)\n",
    "    return img\n",
    "\n",
    "\n",
    "class TaskSolver:        \n",
    "    def train(self, task_train, n_epoch=30):\n",
    "        \"\"\"basic pytorch train loop\"\"\"\n",
    "        self.net = Conv2d(in_channels=10, out_channels=10, kernel_size=5, padding=2)\n",
    "        \n",
    "        criterion = CrossEntropyLoss()\n",
    "        optimizer = Adam(self.net.parameters(), lr = 0.1)\n",
    "        \n",
    "        for epoch in range(n_epoch):\n",
    "            for sample in task_train:\n",
    "                inputs = FloatTensor(inp2img(sample['input'])).unsqueeze(dim=0)\n",
    "                labels = LongTensor(sample['output']).unsqueeze(dim=0)\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.net(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        \n",
    "        return self\n",
    "            \n",
    "    def predict(self, task_test):\n",
    "        predictions = []\n",
    "        with torch.no_grad():\n",
    "            for sample in task_test:\n",
    "                inputs = FloatTensor(inp2img(sample['input'])).unsqueeze(dim=0)\n",
    "                outputs = self.net(inputs)\n",
    "                pred =  outputs.squeeze(dim=0).cpu().numpy().argmax(0)\n",
    "                predictions.append(pred)   \n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_output_shape_is_same(task):\n",
    "    return all([np.array(el['input']).shape == np.array(el['output']).shape for el in task['train']])\n",
    "\n",
    "def calk_score(task_test, predict):\n",
    "    return [int(np.equal(sample['output'], pred).all()) for sample, pred in zip(task_test, predict)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(tasks):\n",
    "    ts = TaskSolver()\n",
    "    result = []\n",
    "    predictions = []\n",
    "    for task in tqdm(tasks):\n",
    "        if input_output_shape_is_same(task):\n",
    "            ts.train(task['train'])\n",
    "            pred = ts.predict(task['test'])\n",
    "            score = calk_score(task['test'], pred)\n",
    "        else:\n",
    "            pred = [el['input'] for el in task['test']]\n",
    "            score = [0]*len(task['test'])\n",
    "        \n",
    "        predictions.append(pred)\n",
    "        result.append(score)\n",
    "    return result, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_result, train_predictions = evaluate(train_tasks)\n",
    "train_solved = [any(score) for score in train_result]\n",
    "\n",
    "total = sum([len(score) for score in train_result])\n",
    "print(f\"solved : {sum(train_solved)} from {total} ({sum(train_solved)/total})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation_result, evaluation_predictions = evaluate(evaluation_tasks)\n",
    "evaluation_solved = [any(score) for score in evaluation_result]\n",
    "\n",
    "total = sum([len(score) for score in evaluation_result])\n",
    "print(f\"solved : {sum(evaluation_solved)} from {total} ({sum(evaluation_solved)/total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise Solved Tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = colors.ListedColormap(\n",
    "        ['#000000', '#0074D9','#FF4136','#2ECC40','#FFDC00',\n",
    "        '#AAAAAA', '#F012BE', '#FF851B', '#7FDBFF', '#870C25'])\n",
    "norm = colors.Normalize(vmin=0, vmax=9)\n",
    "    \n",
    "def plot_pictures(pictures, labels):\n",
    "    fig, axs = plt.subplots(1, len(pictures), figsize=(2*len(pictures),32))\n",
    "    for i, (pict, label) in enumerate(zip(pictures, labels)):\n",
    "        axs[i].imshow(np.array(pict), cmap=cmap, norm=norm)\n",
    "        axs[i].set_title(label)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def plot_sample(sample, predict=None):\n",
    "    if predict is None:\n",
    "        plot_pictures([sample['input'], sample['output']], ['Input', 'Output'])\n",
    "    else:\n",
    "        plot_pictures([sample['input'], sample['output'], predict], ['Input', 'Output', 'Predict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Solved Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for task, prediction, solved in tqdm(zip(evaluation_tasks, evaluation_predictions, evaluation_solved)):\n",
    "    if solved:\n",
    "        for i in range(len(task['train'])):\n",
    "            plot_sample(task['train'][i])\n",
    "            \n",
    "        for i in range(len(task['test'])):\n",
    "            plot_sample(task['test'][i], prediction[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
